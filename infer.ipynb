{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_pth=\"/home/ckq/CHATGPT/niuren/InstructGLM\"\n",
    "old_glm_path=\"/home/ckq/CHATGPT/0model_pretrained/chatglm_3_30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HfArgumentParser\n",
    "import torch\n",
    "import transformers\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "from modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "model = ChatGLMForConditionalGeneration.from_pretrained(old_glm_path, trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_path = project_pth+\"/output/alpaca/chatglm-lora.pt\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "peft_path = \"./output/alpaca/chatglm-lora.pt\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "instructions = json.load(open(project_pth+\"/data/alpaca_data.json\"))\n",
    "\n",
    "instructions = [\n",
    "    {\n",
    "        'instruction': \"下面哪个产品与其他不同\",\n",
    "        \"input\": \"知乎, 百度, 微博, 拼多多\",\n",
    "        \"output\": \"拼多多与其他产品不同，因为它是一个电子商务平台，而知乎、百度和微博都是社交平台/搜索引擎。\",\n",
    "    },\n",
    "    {\n",
    "        'instruction': \"下面哪个产品与其他不同\",\n",
    "        \"input\": \"chatgpt, 文心, CPM, 拼多多\",\n",
    "        \"output\": \"拼多多与其他产品不同，因为它是一家电子商务公司，而ChatGPT、文心和CPM都不是电子商务公司。\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 │   │   │   </span>input_text += <span style=\"color: #808000; text-decoration-color: #808000\">f\"### {</span>idx+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">}.Input:\\n{</span>item[<span style=\"color: #808000; text-decoration-color: #808000\">'input'</span>]<span style=\"color: #808000; text-decoration-color: #808000\">}\\n\\n\"</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 │   │   </span>input_text += <span style=\"color: #808000; text-decoration-color: #808000\">f\"### {</span>idx+<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span><span style=\"color: #808000; text-decoration-color: #808000\">}.Response:\"</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 │   │   </span>batch = tokenizer(input_text, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>10 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>out = model.generate(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 │   │   │   </span>input_ids=batch[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>],                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 │   │   │   </span>attention_mask=torch.ones_like(batch[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span>]).bool(),                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 │   │   │   </span>max_length=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">512</span>,                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">716</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 713 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model.prepare_inputs_for_generation = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generati  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 714 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 715 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(peft_config, PromptLearningConfig):                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 716 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model.generate(**kwargs)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 717 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 718 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> kwargs:                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 719 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids must be provided for Peft model generati</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1437</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1434 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1435 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1436 │   │   │   # 11. run greedy search</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1437 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.greedy_search(                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1438 │   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1439 │   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1440 │   │   │   │   </span>stopping_criteria=stopping_criteria,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2245</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">greedy_search</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2242 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">break</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2243 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2244 │   │   │   # prepare model inputs</span>                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2245 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2246 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2247 │   │   │   # forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2248 │   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/ckq/CHATGPT/niuren/InstructGLM/old_files/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_chatglm.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">973</span> in                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">prepare_inputs_for_generation</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 970 │   │   </span>mask_token = MASK <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> MASK <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> input_ids <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> gMASK                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 971 │   │   </span>use_gmask = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> MASK <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> input_ids <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> gMASK                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 972 │   │   </span>seq = input_ids[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].tolist()                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 973 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>mask_position = seq.index(mask_token)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 974 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 975 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> mask_token <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> seq:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 976 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"You have to add either [MASK] or [gMASK] in your input\"</span>)    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150001</span> is not in list\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m10\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_text += \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m### \u001b[0m\u001b[33m{\u001b[0midx+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m.Input:\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m{\u001b[0mitem[\u001b[33m'\u001b[0m\u001b[33minput\u001b[0m\u001b[33m'\u001b[0m]\u001b[33m}\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0minput_text += \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m### \u001b[0m\u001b[33m{\u001b[0midx+\u001b[94m1\u001b[0m\u001b[33m}\u001b[0m\u001b[33m.Response:\u001b[0m\u001b[33m\"\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   \u001b[0mbatch = tokenizer(input_text, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m10 \u001b[2m│   │   \u001b[0mout = model.generate(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=batch[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m],                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=torch.ones_like(batch[\u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m]).bool(),                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[2m│   │   │   \u001b[0mmax_length=\u001b[94m512\u001b[0m,                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m716\u001b[0m in \u001b[92mgenerate\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 713 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.base_model.prepare_inputs_for_generation = \u001b[96mself\u001b[0m.prepare_inputs_for_generati  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(peft_config, PromptLearningConfig):                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 716 \u001b[2m│   │   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.base_model.generate(**kwargs)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 717 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 718 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33minput_ids\u001b[0m\u001b[33m\"\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m kwargs:                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 719 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33minput_ids must be provided for Peft model generati\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mdecorate_context\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1437\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgenerate\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1434 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1435 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1436 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 11. run greedy search\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1437 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.greedy_search(                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1438 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1439 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1440 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mstopping_criteria=stopping_criteria,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ckq/anaconda3/envs/accel/lib/python3.9/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2245\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mgreedy_search\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2242 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mbreak\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2243 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2244 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# prepare model inputs\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2245 \u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2246 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2247 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2248 \u001b[0m\u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/ckq/CHATGPT/niuren/InstructGLM/old_files/\u001b[0m\u001b[1;33mmodeling_chatglm.py\u001b[0m:\u001b[94m973\u001b[0m in                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mprepare_inputs_for_generation\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 970 \u001b[0m\u001b[2m│   │   \u001b[0mmask_token = MASK \u001b[94mif\u001b[0m MASK \u001b[95min\u001b[0m input_ids \u001b[94melse\u001b[0m gMASK                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 971 \u001b[0m\u001b[2m│   │   \u001b[0muse_gmask = \u001b[94mFalse\u001b[0m \u001b[94mif\u001b[0m MASK \u001b[95min\u001b[0m input_ids \u001b[94melse\u001b[0m gMASK                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 972 \u001b[0m\u001b[2m│   │   \u001b[0mseq = input_ids[\u001b[94m0\u001b[0m].tolist()                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 973 \u001b[2m│   │   \u001b[0mmask_position = seq.index(mask_token)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 974 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 975 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m mask_token \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m seq:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 976 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mYou have to add either [MASK] or [gMASK] in your input\u001b[0m\u001b[33m\"\u001b[0m)    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0m\u001b[1;36m150001\u001b[0m is not in list\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        input_text = f\"### {idx+1}.Instruction:\\n{item['instruction']}\\n\\n\"\n",
    "        if item.get('input'):\n",
    "            input_text += f\"### {idx+1}.Input:\\n{item['input']}\\n\\n\"\n",
    "        input_text += f\"### {idx+1}.Response:\"\n",
    "        batch = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        out = model.generate(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=torch.ones_like(batch[\"input_ids\"]).bool(),\n",
    "            max_length=512,\n",
    "            temperature=0\n",
    "        )# chatGLM报错\n",
    "        out_text = tokenizer.decode(out[0])\n",
    "        answer = out_text.replace(input_text, \"\").replace(\"\\nEND\", \"\").strip()\n",
    "        item['infer_answer'] = answer\n",
    "        print(out_text)\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')\n",
    "        answers.append({'index': idx, **item})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
